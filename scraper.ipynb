{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "+ $d$ = maxmimum degrees of separation\n",
    "+ $c$ = maximum connections per profile  \n",
    "+ $P_e$ = expected number of profiles scraped\n",
    "+ $P_a$ = actual number of profiles scarped\n",
    "\n",
    "Then,  $P_e = \\sum_{i=0}^{d}c^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is frequently the case where $P_a <P_e$. The following are possible explanations:\n",
    "+ The scraper does not work on profiles that have LinkedIn Premium activated. Consequently, when retrieving the related profiles on a given profile, it is designed to ignore the related profiles that have Premium. Let $L_p$ = the number of related profiles that have Premium activated for a given profile $p$\n",
    "+ The scraped also ignores duplicates. If Profile $X$ has already been processed and it is currently scraping Profile $Y$, the scraper will ignore Profile $X$ if it is one of Profile $Y$'s related profiles. Let $D_p$ = the number of related profiles that have already been processed for a given profile $p$\n",
    "\n",
    "+ Consequently, since LinkedIn provides at most 10 related profiles for any given profile, then the number of related profiles retrieved $r_p$ for a given profile $r_p = min(c, 10-L_p-D_p) $\n",
    "+ Additionally, of Profile $A$ has no related profiles, then it is effectively a terminal node, even if its degree of separation from the root is less than $d$: no new profiles are added to the queue for processing, $r_a = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from pprint import PrettyPrinter\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from scraper_utils import *\n",
    "from preprocess_utils import *\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "pp = PrettyPrinter(indent = 4)\n",
    "class LI_Scraper:\n",
    "    def __init__(self, ref_path = \"path.txt\"):\n",
    "            \n",
    "        self.ref_path = ref_path\n",
    "        self.col_dict = {col:[] for col in [\"source_id\", \"dest_id\", \"dest_name\", \\\n",
    "            \"dest_title\", \"dest_company\", \"dest_location\", \"dest_school\", \"dest_pic\", \"dest_childs\", \"tree_degree\"]}\n",
    "    \n",
    "        self.df = pd.DataFrame(self.col_dict)\n",
    "            \n",
    "        self.processed = set() \n",
    "        self.browser = None\n",
    "    \n",
    "    def _setup_browser(self):     \n",
    "        with open(self.ref_path, \"r\") as f:\n",
    "            path, username, password = (x.strip(\"\\n\") for x in f)\n",
    "\n",
    "        browser = webdriver.Chrome(path)\n",
    "        browser.get(\"https://www.linkedin.com/login/\")\n",
    "\n",
    "        #wait for webpage to load\n",
    "        \n",
    "        sleep(3)\n",
    "        user = browser.find_elements_by_name(\"session_key\")[0]\n",
    "        user.send_keys(username)\n",
    "\n",
    "        pwd = browser.find_elements_by_name(\"session_password\")[0]\n",
    "        pwd.send_keys(password)\n",
    "\n",
    "        login = browser.find_elements_by_xpath(\"//button[@type = 'submit']\")[0]\n",
    "        login.click()\n",
    "        self.browser = browser\n",
    "    \n",
    "    def _get_header_info(self):\n",
    "        header_section = self.browser.find_element_by_xpath(\"//section[@class = 'pv-top-card artdeco-card ember-view']\")\n",
    "        name = header_section.find_element_by_xpath(\".//li[@class = 'inline t-24 t-black t-normal break-words']\").text\n",
    "        \n",
    "        try:\n",
    "            pic = header_section.find_element_by_xpath(\".//img[@class = 'pv-top-card__photo presence-entity__image EntityPhoto-circle-9 lazy-image loaded ember-view']\").get_attribute(\"src\")\n",
    "        except NoSuchElementException:\n",
    "            pic = None\n",
    "        \n",
    "        return (name, pic)\n",
    "    \n",
    "    def _get_background_info(self):\n",
    "        bg = self.browser.find_element_by_id(\"oc-background-section\").text\n",
    "        data = []\n",
    "        try:\n",
    "            title = parse(bg, \"\\nTitle\")\n",
    "        except LookupError:\n",
    "            try:\n",
    "                title = parse(bg, \"Experience\")\n",
    "            except LookupError:\n",
    "                title = None\n",
    "        data.append(title)\n",
    "        \n",
    "        for q in [\"\\nCompany Name\", \"\\nLocation\", \"\\nEducation\"]:\n",
    "            try:\n",
    "                data.append(parse(bg, q))\n",
    "            except LookupError:\n",
    "                data.append(None)\n",
    "        return data  \n",
    "    \n",
    "    def _get_related_profiles(self, source_id, n, get_links):\n",
    "        data = [None, None]\n",
    "        try:\n",
    "            check = WebDriverWait(self.browser, 30).until(EC.presence_of_element_located((By.XPATH, \"//h2[@class = 't-16 t-black t-normal']\")))    \n",
    "            assert check.text.strip().lower() == \"people also viewed\"\n",
    "\n",
    "        except (NoSuchElementException, TimeoutException, AssertionError):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            related_list = list(enumerate(self.browser.find_elements_by_xpath(\".//li[@class = 'pv-browsemap-section__member-container mt4 ember-view']\")))\n",
    "            all_links = [r[1].find_element_by_xpath(\".//a[@data-control-name = 'browsemap_profile']\").get_attribute(\"href\") for r in related_list]\n",
    "            all_ids =[extract_id(l) for l in all_links]\n",
    "\n",
    "            links = []\n",
    "            if get_links:\n",
    "                print(\"Getting links\", end = \"...\")\n",
    "                cnt = 0\n",
    "                while related_list and cnt<n:\n",
    "                    r = related_list.pop(0)\n",
    "                    rp = r[1].find_element_by_xpath(\".//a[@data-control-name = 'browsemap_profile']\")\n",
    "\n",
    "                    #cant scrape profiles that have premium for some reason\n",
    "                    if rp.find_elements_by_class_name(\"premium-icon\"):\n",
    "                        continue\n",
    "                    else:\n",
    "                        i = r[0]\n",
    "                        if all_ids[i] not in self.processed:\n",
    "                            links.append(all_links[i])\n",
    "                            cnt += 1                        \n",
    "                data[1] = {\"source\":source_id, \"dest\":links}\n",
    "\n",
    "            id_str =  \",\".join(all_ids)\n",
    "            data[0] = id_str\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return data\n",
    "        \n",
    "    \n",
    "    def _scrape_profile(self, url, n, source_id, degree, get_links):\n",
    "        profile_dict = self.col_dict.copy()\n",
    "        \n",
    "        profile_dict[\"source_id\"] = source_id\n",
    "        dest_id = extract_id(url)\n",
    "\n",
    "        profile_dict[\"dest_id\"] = dest_id\n",
    "        profile_dict[\"tree_degree\"] = degree   \n",
    "        \n",
    "        \n",
    "        print(f\"\\t\\tRetrieving data from [ {url} ]\", end = \"...\")\n",
    "        self.browser.get(url)\n",
    "        \n",
    "        profile_dict[\"dest_name\"], profile_dict[\"dest_pic\"] = self._get_header_info()\n",
    "        profile_dict[\"dest_title\"], profile_dict[\"dest_company\"], profile_dict[\"dest_location\"], profile_dict[\"dest_school\"] = self._get_background_info()\n",
    "\n",
    "        data = self._get_related_profiles(url, n, get_links)\n",
    "        profile_dict[\"dest_childs\"] = data[0]\n",
    "        \n",
    "        self.df = self.df.append(profile_dict, ignore_index = True)\n",
    "\n",
    "        self.processed.add(dest_id)\n",
    "        return data[1]\n",
    "    \n",
    "    def _find_inters(self):\n",
    "        l = []\n",
    "        for i in range(len(self.df)):\n",
    "            str_childs = self.df.loc[i, \"dest_childs\"]\n",
    "            if str_childs:\n",
    "                s = set(str_childs.split(\",\"))\n",
    "                inter = \",\".join(self.processed.intersection(s))\n",
    "                if not inter: #no intersection\n",
    "                    l.append(None)\n",
    "                else:\n",
    "                    l.append(inter)\n",
    "            else:\n",
    "                l.append(None)\n",
    "        return l \n",
    "    \n",
    "    def run(self, start_url, degrees = 3, connects = 3, save = True, csv_path = \"profiles.csv\", process = True):\n",
    "        #set up browser object\n",
    "        start_time = datetime.now()\n",
    "        connects = min(connects, 10) #only 10 related profiles displayed\n",
    "        if not self.browser:\n",
    "            print(\"Setting up browser\", end = \"...\")\n",
    "            self._setup_browser()\n",
    "            \n",
    "            if not self.browser:\n",
    "                return\n",
    "            \n",
    "            print(\"Done\")\n",
    "            print()\n",
    "        print(\"Running profile scraper:\")\n",
    "        #bfs on connects\n",
    "        q = deque()\n",
    "        \n",
    "        #track depth, \n",
    "        print(\"\\tInitial profile\")\n",
    "        q.append(self._scrape_profile(start_url, connects, None, 0, True))\n",
    "        depth = 1\n",
    "        q.append(None)\n",
    "        \n",
    "        get_links = True\n",
    "        \n",
    "        while len(q) > 0:\n",
    "            urls = q.popleft()\n",
    "            if urls is None:\n",
    "                depth += 1\n",
    "                if depth >= degrees:\n",
    "                    get_links = False\n",
    "                q.append(None)\n",
    "                urls = q.popleft()\n",
    "            if urls is None:\n",
    "                end_time = datetime.now()\n",
    "                print(f\"Profile scraping finished in {str(end_time - start_time)}\")\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                parent_link = urls[\"source\"]\n",
    "                parent = extract_id(parent_link)\n",
    "                \n",
    "                dest_links = urls[\"dest\"]\n",
    "                print(f\"\\n\\tDegree of separation: {depth}\", end = f\"\\t\\tLinked from: {parent}\\n\")\n",
    "\n",
    "\n",
    "                if not dest_links:\n",
    "                    print(f\"\\t\\t{parent} has no related profiles\")\n",
    "                else:\n",
    "                    for link in urls[\"dest\"]:\n",
    "                        if extract_id(link) not in self.processed:\n",
    "                            related = self._scrape_profile(link, connects, parent, depth, get_links)\n",
    "                            if related is not None:\n",
    "                                q.append(related)\n",
    "        \n",
    "        self.df[\"dest_connected\"] = self._find_inters()\n",
    "        \n",
    "        if process:\n",
    "            print(\"\\nPreprocessing DataFrame\", end = \"...\")\n",
    "            self.df = preprocess(self.df)\n",
    "            print(\"Done\")\n",
    "        if save:\n",
    "            self.save(csv_path)\n",
    "            \n",
    "    def save(self, csv_path = \"profiles.csv\"):\n",
    "        print(f\"Saving DataFrame to {csv_path}\")\n",
    "        self.df.to_csv(csv_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondade3031e102d342aa8a702b3f5f29a489"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
